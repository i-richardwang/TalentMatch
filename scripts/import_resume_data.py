import os
import sys
import json
from typing import Dict, List, Any

# Add project root directory to path
project_root = os.path.dirname(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import asyncio
from utils.database.connections import MySQLConnectionManager
from backend.resume_management.storage.resume_repository import ResumeRepository
from backend.resume_management.storage.resume_vector_storage import store_resumes_batch_in_milvus, store_resumes_batch_in_milvus_async

def init_database():
    """Initialize database and tables"""
    try:
        ResumeRepository.init_all_tables()
        print("æ•°æ®åº“è¡¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"åˆå§‹åŒ–æ•°æ®åº“æ—¶å‡ºé”™: {e}")

def load_json_data(file_path: str) -> List[Dict[str, Any]]:
    """Load JSON file data"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def merge_resume_data(resume_data: List[Dict], summary_data: List[Dict]) -> List[Dict]:
    """Merge resume data and summary data"""
    summary_dict = {}
    for item in summary_data:
        resume_id = item['id']
        # Handle data nested in ResumeSummary
        if 'ResumeSummary' in item:
            summary = item['ResumeSummary']
        else:
            summary = item
            
        # Use get method to safely retrieve fields with default values
        summary_dict[resume_id] = {
            'characteristics': summary.get('characteristics', ''),
            'experience': summary.get('experience', ''),
            'skills_overview': summary.get('skills_overview', '')
        }
    
    merged_data = []
    for resume in resume_data:
        try:
            resume_id = resume.get('id')
            if not resume_id:
                print(f"è­¦å‘Šï¼šè·³è¿‡ä¸€æ¡æ²¡æœ‰IDçš„ç®€å†æ•°æ®")
                continue
                
            if resume_id in summary_dict:
                summary = summary_dict[resume_id]
                
                # Safely retrieve and process all fields
                personal_info = resume.get('personal_info', {})
                education = resume.get('education', [])
                work_experiences = resume.get('work_experiences', [])
                project_experiences = resume.get('project_experiences', [])
                
                merged_item = {
                    'resume_id': resume_id,
                    'personal_info': json.dumps(personal_info),
                    'education': json.dumps(education),
                    'work_experiences': json.dumps(work_experiences),
                    'project_experiences': json.dumps(project_experiences),
                    'characteristics': summary.get('characteristics', ''),
                    'experience_summary': summary.get('experience', ''),
                    'skills_overview': summary.get('skills_overview', ''),
                    'resume_format': 'json',
                    'file_or_url': 'data/datasets/merged_resume_json.json'
                }
                merged_data.append(merged_item)
            else:
                print(f"è­¦å‘Šï¼šç®€å†ID {resume_id} åœ¨æ‘˜è¦æ•°æ®ä¸­æœªæ‰¾åˆ°å¯¹åº”è®°å½•")
        except Exception as e:
            print(f"è­¦å‘Šï¼šå¤„ç†ç®€å†æ—¶å‡ºé”™ (ID: {resume.get('id', 'unknown')}): {str(e)}")
            continue
    
    return merged_data

def import_to_database(data: List[Dict], batch_size: int = 500, 
                      import_mysql: bool = True, import_milvus: bool = True):
    """
    Import data to database with batch processing for better performance
    
    Args:
        data: List of resume data
        batch_size: Number of resumes to process in each batch (default: 500)
        import_mysql: Whether to import to MySQL (default: True)
        import_milvus: Whether to import to Milvus (default: True)
    """
    if not import_mysql and not import_milvus:
        print("âš ï¸  è‡³å°‘éœ€è¦é€‰æ‹©ä¸€ä¸ªå¯¼å…¥ç›®æ ‡ï¼ˆMySQL æˆ– Milvusï¼‰")
        return
    try:
        # Convert key names to new format
        processed_data = []
        for item in data:
            processed_item = {
                'id': item['resume_id'],  # Convert key name
                'personal_info': json.loads(item['personal_info']) if isinstance(item['personal_info'], str) else item['personal_info'],
                'education': json.loads(item['education']) if isinstance(item['education'], str) else item['education'],
                'work_experiences': json.loads(item['work_experiences']) if isinstance(item['work_experiences'], str) else item['work_experiences'],
                'project_experiences': json.loads(item['project_experiences']) if isinstance(item['project_experiences'], str) else item['project_experiences'],
                'characteristics': item['characteristics'],
                'experience_summary': item['experience_summary'],
                'skills_overview': item['skills_overview'],
                'resume_format': item['resume_format'],
                'file_or_url': item['file_or_url']
            }
            processed_data.append(processed_item)
        
        print(f"\nå¼€å§‹æ‰¹é‡å¯¼å…¥ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"æ€»æ•°æ®é‡: {len(processed_data)} æ¡")
        print(f"é¢„è®¡æ‰¹æ¬¡æ•°: {(len(processed_data) + batch_size - 1) // batch_size}")
        
        if import_mysql:
            print(f"âœ“ å°†å¯¼å…¥åˆ° MySQL")
        if import_milvus:
            print(f"âœ“ å°†å¯¼å…¥åˆ° Milvus")
        
        print("-" * 60)
        
        # Store to MySQL in batches
        mysql_success_count = 0
        if import_mysql:
            print("\nğŸ“Š ç¬¬1é˜¶æ®µï¼šæ‰¹é‡å¯¼å…¥åˆ° MySQL...")
            
            for batch_idx in range(0, len(processed_data), batch_size):
                batch = processed_data[batch_idx:batch_idx + batch_size]
                batch_num = batch_idx // batch_size + 1
                total_batches = (len(processed_data) + batch_size - 1) // batch_size
                
                try:
                    success = ResumeRepository.batch_store_full_resumes(batch)
                    mysql_success_count += success
                    print(f"  æ‰¹æ¬¡ {batch_num}/{total_batches}: âœ… {success}/{len(batch)} æ¡")
                except Exception as e:
                    print(f"  æ‰¹æ¬¡ {batch_num}/{total_batches}: âŒ å¤±è´¥ - {str(e)}")
            
            print(f"\nâœ… MySQLæ‰¹é‡å¯¼å…¥å®Œæˆ: {mysql_success_count}/{len(processed_data)} æ¡")
            print("-" * 60)
        else:
            print("\nâŠ˜ è·³è¿‡ MySQL å¯¼å…¥")
            print("-" * 60)
        
        # Store to Milvus vector database in batches
        vector_success_count = 0
        total_failed = []
        
        if import_milvus:
            print("\nğŸ” ç¬¬2é˜¶æ®µï¼šæ‰¹é‡å¯¼å…¥åˆ° Milvus å‘é‡æ•°æ®åº“ (ä½¿ç”¨å¼‚æ­¥åŠ é€Ÿâš¡)...")
            
            async def process_all_batches():
                """Process all batches asynchronously"""
                nonlocal vector_success_count, total_failed
                
                for batch_idx in range(0, len(processed_data), batch_size):
                    batch = processed_data[batch_idx:batch_idx + batch_size]
                    batch_num = batch_idx // batch_size + 1
                    total_batches = (len(processed_data) + batch_size - 1) // batch_size
                    
                    print(f"\n  æ‰¹æ¬¡ {batch_num}/{total_batches} (ç®€å† {batch_idx+1}-{min(batch_idx+batch_size, len(processed_data))})")
                    
                    try:
                        # Use async version for faster processing (10 concurrent requests)
                        success, failed = await store_resumes_batch_in_milvus_async(batch)
                        vector_success_count += success
                        total_failed.extend(failed)
                        
                        print(f"    âœ… æˆåŠŸ: {success}/{len(batch)} æ¡")
                        if failed:
                            print(f"    âš ï¸  å¤±è´¥: {len(failed)} æ¡")
                            
                    except Exception as e:
                        print(f"    âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}")
                        for item in batch:
                            total_failed.append({'id': item.get('id'), 'error': str(e)})
            
            # Run async processing
            asyncio.run(process_all_batches())
        else:
            print("\nâŠ˜ è·³è¿‡ Milvus å‘é‡å¯¼å…¥")
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š å¯¼å…¥å®Œæˆç»Ÿè®¡:")
        if import_mysql:
            print(f"  MySQL: {mysql_success_count}/{len(data)} æ¡")
        if import_milvus:
            print(f"  Milvuså‘é‡: {vector_success_count}/{len(data)} æ¡")
        
        if import_milvus and total_failed:
            print(f"\nâš ï¸  å¤±è´¥è¯¦æƒ…: {len(total_failed)} æ¡ç®€å†")
            print(f"  å¤±è´¥ç®€å†ID: {[f['id'] for f in total_failed[:10]]}")
            if len(total_failed) > 10:
                print(f"  ... ä»¥åŠå…¶ä»– {len(total_failed) - 10} æ¡")
        elif import_milvus and not total_failed:
            print(f"\nğŸ‰ æ‰€æœ‰ç®€å†å‘é‡æ•°æ®å¯¼å…¥æˆåŠŸï¼")
        
        print("=" * 60)

    except Exception as e:
        print(f"å¯¼å…¥æ•°æ®æ—¶å‡ºé”™: {e}")

def main():
    import sys
    
    # Parse command line arguments for selective import
    import_mysql = True
    import_milvus = True
    
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        if mode == 'mysql':
            import_milvus = False
            print("ğŸ¯ æ¨¡å¼ï¼šä»…å¯¼å…¥ MySQL")
        elif mode == 'milvus':
            import_mysql = False
            print("ğŸ¯ æ¨¡å¼ï¼šä»…å¯¼å…¥ Milvus")
        elif mode == 'all':
            print("ğŸ¯ æ¨¡å¼ï¼šå¯¼å…¥ MySQL + Milvus")
        else:
            print("âš ï¸  æœªçŸ¥æ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤ï¼ˆå¯¼å…¥å…¨éƒ¨ï¼‰")
            print("æç¤ºï¼šä½¿ç”¨ 'mysql', 'milvus' æˆ– 'all' å‚æ•°")
    else:
        print("ğŸ¯ æ¨¡å¼ï¼šå¯¼å…¥ MySQL + Milvusï¼ˆé»˜è®¤ï¼‰")
        print("æç¤ºï¼šå¯ä½¿ç”¨å‚æ•°æŒ‡å®šæ¨¡å¼ - python script.py [mysql|milvus|all]")
    
    print()
    
    # Initialize database
    init_database()

    try:
        # Load data
        print("æ­£åœ¨åŠ è½½ç®€å†æ•°æ®...")
        resume_data = load_json_data('data/datasets/merged_resume_json.json')
        print(f"æˆåŠŸåŠ è½½ {len(resume_data)} æ¡ç®€å†æ•°æ®")
        
        print("æ­£åœ¨åŠ è½½ç®€å†æ‘˜è¦æ•°æ®...")
        summary_data = load_json_data('data/datasets/resume_summary.json')
        print(f"æˆåŠŸåŠ è½½ {len(summary_data)} æ¡æ‘˜è¦æ•°æ®")

        # Merge data
        print("æ­£åœ¨åˆå¹¶æ•°æ®...")
        merged_data = merge_resume_data(resume_data, summary_data)
        print(f"æˆåŠŸåˆå¹¶ {len(merged_data)} æ¡æ•°æ®")

        # Import to database with selected mode
        import_to_database(merged_data, import_mysql=import_mysql, import_milvus=import_milvus)
    except FileNotFoundError as e:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ - {e}")
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯ï¼šJSON æ ¼å¼ä¸æ­£ç¡® - {e}")
    except Exception as e:
        print(f"é”™è¯¯ï¼šå¤„ç†æ•°æ®æ—¶å‡ºç°é—®é¢˜ - {e}")

if __name__ == "__main__":
    main() 
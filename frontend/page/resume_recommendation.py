import streamlit as st
import sys
import os
import pandas as pd
import uuid
import asyncio
import tempfile
from pathlib import Path

# Add project root directory to Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)


from langchain_core.globals import set_llm_cache
from langchain_community.cache import SQLiteCache

# Setup LLM cache with fallback to temp directory for cloud deployment
def setup_llm_cache():
    """
    Setup LLM cache with support for both local and cloud deployment.
    - Local: uses data/llm_cache/langchain.db
    - Cloud (Streamlit): uses system temp directory
    """
    # Try local cache directory first
    local_cache_dir = Path(project_root) / "data" / "llm_cache"
    local_cache_path = local_cache_dir / "langchain.db"

    try:
        # Ensure directory exists and is writable
        local_cache_dir.mkdir(parents=True, exist_ok=True)
        # Test write permission by creating a test file
        test_file = local_cache_dir / ".write_test"
        test_file.touch()
        test_file.unlink()

        # If successful, use local cache
        cache_path = local_cache_path
    except (OSError, PermissionError):
        # Fallback to system temp directory (for Streamlit Cloud)
        temp_cache_dir = Path(tempfile.gettempdir()) / "talentmatch_cache"
        temp_cache_dir.mkdir(exist_ok=True)
        cache_path = temp_cache_dir / "langchain.db"

    set_llm_cache(SQLiteCache(database_path=str(cache_path)))

setup_llm_cache()

from backend.resume_management.recommendation.resume_recommender import (
    ResumeRecommender,
)
from frontend.ui_components import apply_common_styles, display_project_info

# Global variables
chat_container = None


def display_hr_insights():
    """
    Display the value and application scenarios of talent recommendation.
    """
    with st.expander("ğŸ’¡ About", expanded=True):
        st.markdown("""
        é€šè¿‡ AI è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¿«é€ŸåŒ¹é…æœ€åˆé€‚çš„å€™é€‰äººã€‚ä»æŠ€èƒ½ã€å·¥ä½œç»éªŒã€æ•™è‚²èƒŒæ™¯ç­‰å¤šä¸ªç»´åº¦æ™ºèƒ½åˆ†æï¼Œ
        è‡ªåŠ¨ç”Ÿæˆæ¨èç†ç”±ï¼Œå¤§å¹…æå‡æ‹›è˜æ•ˆç‡ã€‚

        **é€‚ç”¨åœºæ™¯**ï¼šç®€å†åˆç­›ã€å€™é€‰äººæœç´¢

        **ğŸ’¡ ä½¿ç”¨æç¤º**ï¼šç›´æ¥ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‹›è˜éœ€æ±‚å³å¯ï¼Œä¾‹å¦‚ï¼š
        - "éœ€è¦ä¸€ä½æœ‰ 5 å¹´ä»¥ä¸Šç»éªŒçš„ Python åç«¯å·¥ç¨‹å¸ˆ"
        - "æ‰¾ä¸€ä¸ªæ•°æ®åˆ†æå¸ˆï¼Œç†Ÿæ‚‰ SQL å’Œæ•°æ®å¯è§†åŒ–"
        - "æ¨èå‡ ä½æœ‰æœºå™¨å­¦ä¹ é¡¹ç›®ç»éªŒçš„ç®—æ³•å·¥ç¨‹å¸ˆ"
        """)


def display_chat_history():
    """Display chat history"""
    with chat_container.container():
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                if isinstance(msg["content"], str):
                    st.write(msg["content"])
                elif isinstance(msg["content"], dict):
                    if msg["content"]["type"] == "search_strategy":
                        st.write("æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä»¥ä¸‹æ£€ç´¢ç­–ç•¥ï¼š")
                        st.table(msg["content"]["data"])
                    elif msg["content"]["type"] == "recommendations":
                        st.write("ä»¥ä¸‹æ˜¯æ ¹æ®æ‚¨çš„éœ€æ±‚æ¨èçš„ç®€å†ï¼š")
                        display_recommendations(msg["content"]["data"])


def display_recommendations(recommendations):
    """Display recommended resumes"""
    for idx, rec in enumerate(recommendations, 1):
        with st.expander(
            f"æ¨è {idx}: ç®€å†ID {rec['ç®€å†ID']} (æ€»åˆ†: {rec['æ€»åˆ†']:.2f})"
        ):
            col1, col2 = st.columns([2, 5])

            with col1:
                st.markdown("#### æ¨èç†ç”±")
                st.info(rec["æ¨èç†ç”±"])

            with col2:
                st.markdown("#### ä¸ªäººæ¦‚å†µ")
                st.write(rec["ä¸ªäººç‰¹å¾"])

                st.markdown("#### å·¥ä½œç»éªŒ")
                st.write(rec["å·¥ä½œç»éªŒ"])

                st.markdown("#### æŠ€èƒ½æ¦‚è§ˆ")
                st.write(rec["æŠ€èƒ½æ¦‚è§ˆ"])


async def process_user_input(prompt: str):
    """Process user input"""
    st.session_state.messages.append({"role": "user", "content": prompt})
    display_chat_history()

    if st.session_state.current_stage == "initial_query":
        with st.spinner("æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚..."):
            status = await st.session_state.recommender.process_query(
                prompt, st.session_state.session_id
            )
        st.session_state.current_stage = (
            "refining_query"
            if status == "need_more_info"
            else "generating_recommendations"
        )
    elif st.session_state.current_stage == "refining_query":
        with st.spinner("æ­£åœ¨å¤„ç†æ‚¨çš„å›ç­”..."):
            status = await st.session_state.recommender.process_answer(
                prompt, st.session_state.session_id
            )
        if status == "ready":
            st.session_state.current_stage = "generating_recommendations"

    next_question = st.session_state.recommender.get_next_question()
    if next_question:
        st.session_state.messages.append(
            {"role": "assistant", "content": next_question}
        )
        display_chat_history()
    elif st.session_state.current_stage == "generating_recommendations":
        refined_query = st.session_state.recommender.get_refined_query()
        if refined_query:
            st.session_state.refined_query = refined_query
            st.session_state.messages.append(
                {
                    "role": "assistant",
                    "content": f"æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æ€»ç»“å‡ºä»¥ä¸‹æ‹›è˜æè¿°ï¼š\n\n{refined_query}",
                }
            )
            display_chat_history()

        st.session_state.processing = True
        st.session_state.strategy_displayed = False


def main():
    """Main function to run the intelligent resume recommendation system"""
    # Initialize session state
    if "recommender" not in st.session_state:
        st.session_state.recommender = ResumeRecommender()
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ™ºèƒ½ç®€å†æ¨èåŠ©æ‰‹ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨çš„æ‹›è˜éœ€æ±‚ã€‚",
            }
        ]
        st.session_state.current_stage = "initial_query"
        st.session_state.search_strategy = None
        st.session_state.recommendations = None
        st.session_state.processing = False
        st.session_state.strategy_displayed = False
        st.session_state.refined_query = None
        st.session_state.top_n = 3  # Default recommendation count
        st.session_state.session_id = str(uuid.uuid4())

    # Apply custom styles
    apply_common_styles()

    # Main interface
    st.title("ğŸ‘¥ æ™ºèƒ½äººæ‰æ¨è")
    st.markdown("---")
    
    # Display project info
    display_project_info()

    # Display value and application scenarios
    display_hr_insights()

    st.markdown("## äººæ‰æ¨è")

    # Add advanced settings
    with st.expander("é«˜çº§è®¾ç½®", expanded=False):
        st.session_state.top_n = st.number_input(
            "æ¨èç®€å†æ•°é‡", min_value=1, max_value=10, value=st.session_state.top_n
        )

    # Create a container to display chat history
    global chat_container
    chat_container = st.empty()

    # Initial display of chat history
    display_chat_history()

    # Handle user input
    if prompt := st.chat_input("è¾“å…¥æ‚¨çš„éœ€æ±‚æˆ–å›ç­”"):
        asyncio.run(process_user_input(prompt))

    # Handle recommendation generation process
    if st.session_state.processing:

        async def generate_recommendations():
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ•´ä½“ç®€å†æœç´¢ç­–ç•¥..."):
                await st.session_state.recommender.generate_overall_search_strategy(
                    st.session_state.session_id
                )

            # Display overall search strategy
            collection_relevances = (
                st.session_state.recommender.get_overall_search_strategy()
            )
            if collection_relevances and not st.session_state.strategy_displayed:
                dimension_descriptions = {
                    "work_experiences": "å·¥ä½œç»å†",
                    "skills": "ä¸“ä¸šæŠ€èƒ½",
                    "educations": "æ•™è‚²èƒŒæ™¯",
                    "project_experiences": "é¡¹ç›®ç»éªŒ",
                    "personal_infos": "ä¸ªäººæ¦‚å†µ",
                }
                table_data = [
                    {
                        "ç»´åº¦": dimension_descriptions.get(
                            relevance["collection_name"], relevance["collection_name"]
                        ),
                        "é‡è¦ç¨‹åº¦": f"{relevance['relevance_score'] * 100:.0f}%",
                    }
                    for relevance in collection_relevances
                ]
                st.session_state.search_strategy = pd.DataFrame(table_data)

                strategy_message = {
                    "type": "search_strategy",
                    "data": st.session_state.search_strategy,
                }
                st.session_state.messages.append(
                    {"role": "assistant", "content": strategy_message}
                )
                display_chat_history()
                st.session_state.strategy_displayed = True

            with st.spinner("æ­£åœ¨ç”Ÿæˆè¯¦ç»†çš„æ£€ç´¢ç­–ç•¥..."):
                await st.session_state.recommender.generate_detailed_search_strategy(
                    st.session_state.session_id
                )

            with st.spinner("æ­£åœ¨è®¡ç®—ç®€å†å¾—åˆ†..."):
                await st.session_state.recommender.calculate_resume_scores(
                    st.session_state.top_n
                )

            with st.spinner("æ­£åœ¨è·å–ç®€å†è¯¦ç»†ä¿¡æ¯..."):
                st.session_state.recommender.resume_details = await st.session_state.recommender.output_generator.fetch_resume_details(
                    st.session_state.recommender.ranked_resume_scores
                )

            with st.spinner("æ­£åœ¨ç”Ÿæˆæ¨èç†ç”±..."):
                await st.session_state.recommender.generate_recommendation_reasons(
                    st.session_state.session_id
                )

            with st.spinner("æ­£åœ¨å‡†å¤‡æœ€ç»ˆæ¨èç»“æœ..."):
                await st.session_state.recommender.prepare_final_recommendations()

            # Update recommendation results
            recommendations = st.session_state.recommender.get_recommendations()
            if recommendations:
                st.session_state.recommendations = recommendations

                recommendation_message = {
                    "type": "recommendations",
                    "data": recommendations,
                }

                st.session_state.messages.append(
                    {"role": "assistant", "content": recommendation_message}
                )
                display_chat_history()

                st.info(
                    f"ä»¥ä¸Šæ˜¯ä¸ºæ‚¨æ¨èçš„ {len(recommendations)} ä»½ç®€å†ï¼Œæ‚¨å¯ä»¥å±•å¼€æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ã€‚å¦‚éœ€è¿›è¡Œæ–°çš„æŸ¥è¯¢ï¼Œè¯·åœ¨ä¸‹æ–¹è¾“å…¥æ¡†ä¸­è¾“å…¥æ–°çš„éœ€æ±‚ã€‚"
                )
            else:
                st.warning(
                    "æŠ±æ­‰ï¼Œæˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ‚¨è¦æ±‚çš„ç®€å†ã€‚æ‚¨å¯ä»¥å°è¯•è°ƒæ•´ä¸€ä¸‹éœ€æ±‚å†è¯•è¯•ã€‚"
                )

            st.session_state.current_stage = "initial_query"
            st.session_state.processing = False
            st.session_state.strategy_displayed = False

        asyncio.run(generate_recommendations())


if __name__ == "__main__":
    main()
